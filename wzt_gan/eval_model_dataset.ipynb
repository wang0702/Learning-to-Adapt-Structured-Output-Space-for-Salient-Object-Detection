{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from cv2 import imread\n",
    "from os import popen\n",
    "from nydataloader import EvalDataset\n",
    "from evaluator import Eval_thread\n",
    "import pickle\n",
    "import torch\n",
    "import random\n",
    "# 设置使用哪张卡\n",
    "torch.cuda.set_device(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用说明\n",
    "\n",
    "1. 规范目录结构\n",
    "   DUT-OMRON、DUTS-7-8等对应下面代码中的dataset_names\n",
    "   gt_dir和pred_dir都要有dataset_names说明的这些文件夹\n",
    "   pred_dir对应的是预测结果文件夹的根目录，以里面的一个子文件夹train-mode-semi-4_result为例\n",
    "   完整结构如下图所示\n",
    "```\n",
    "|-------pred_dir\n",
    "    ├── train-mode-semi-4_result\n",
    "    │   ├── DUT-OMRON\n",
    "    │   ├── DUTS-7-8\n",
    "    │   ├── DUTS-TE\n",
    "    │   ├── ECSSD\n",
    "    │   ├── HKU-IS\n",
    "    │   ├── PASCAL-S\n",
    "    │   └── SOD\n",
    "    \n",
    "    ├── train-mode-semi-5_result\n",
    "    │   ├── DUT-OMRON\n",
    "    │   ├── DUTS-7-8\n",
    "    │   ├── DUTS-TE\n",
    "    │   ├── ECSSD\n",
    "    │   ├── HKU-IS\n",
    "    │   ├── PASCAL-S\n",
    "    │   └── SOD\n",
    "    \n",
    "    ├── ........\n",
    "    │   ├── DUT-OMRON\n",
    "    │   ├── DUTS-7-8\n",
    "    │   ├── DUTS-TE\n",
    "    │   ├── ECSSD\n",
    "    │   ├── HKU-IS\n",
    "    │   ├── PASCAL-S\n",
    "    │   └── SOD\n",
    "```\n",
    "   gt_dir目录结构如下图所示，这里建议使用软链接链接过去\n",
    "```\n",
    "├── gt\n",
    "│   ├── DUT-OMRON -> /home/ai-server/disk2/thb/dataset/DUT-OMRON/GT/\n",
    "│   ├── DUTS-7-8 -> /home/ai-server/disk2/thb/dataset/DUTS/DUTS-TE/TEST-7-8/GT\n",
    "│   ├── DUTS-TE -> /home/ai-server/disk2/thb/dataset/DUTS/DUTS-TE/DUTS-TE-Mask/\n",
    "│   ├── ECSSD -> /home/ai-server/disk2/thb/dataset/ECSSD/GT/\n",
    "│   ├── HKU-IS -> /home/ai-server/disk2/thb/dataset/HKU-IS/GT/\n",
    "│   ├── PASCAL-S -> /home/ai-server/disk2/thb/dataset/PASCAL-S/GT/\n",
    "│   └── SOD -> /home/ai-server/disk2/thb/dataset/SOD/GT/\n",
    "```\n",
    "\n",
    "2. 多个模型对应多个预测结果\n",
    "    model_names即对应预测结果文件夹，这是一个可迭代的元素\n",
    "    \n",
    "3. salmetric_cmd_path 对应C++版本的测试程序\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_names = ('DUTS-7-8','DUT-OMRON','DUTS-TE','ECSSD','HKU-IS','PASCAL-S','SOD')\n",
    "dataset_names = ('DUT-OMRON','DUTS-TE','ECSSD','HKU-IS','PASCAL-S','SOD')\n",
    "# dataset_names = ('DUTS-TE',)\n",
    "gt_dir = '/home/ai-server/disk2/thb/pycode/Evaluation-SOD/data-2/gt'\n",
    "# model_name is in the pred_dir  \n",
    "pred_dir = '/home/thb/MyAICode/pycode/AdvSaliency/'\n",
    "\n",
    "salmetric_cmd_path = \"/home/thb/MyAICode/dataset/DUTS/DUTS-TE/salmetric\"\n",
    "lst_file_path = './tmp01-%d.lst' % random.randint()\n",
    "# model_names = ['d-learning-rate_%s_result' % s for s in (\"0.0009\",\"0.001\",)]\n",
    "# model_names = ['learning-rate_%s_result' % i for i in ('0.0001','0.0006','0.0009') ]\n",
    "# model_names = ['train-without-D_result','train-without-semi_result','train-without-adv_result','train-with-dmap_loss_result']\n",
    "model_names = ['train-without-D-%d_result' % i for i in (1000,2000)]\n",
    "res_dict = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_datasets = [ os.path.join(gt_dir,dataset_name) for dataset_name in dataset_names ]\n",
    "gt_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_model_dirs = [os.path.join(pred_dir,model_name) for model_name in model_names] \n",
    "pred_model_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_dir_exists(gt_dir,pred_dir):\n",
    "    # get max-s\n",
    "    image_path_list = list()\n",
    "    label_path_list = list()\n",
    "    \n",
    "    for image_name in os.listdir(gt_dir):\n",
    "        label_path = os.path.join(gt_dir, image_name)\n",
    "        image_path = os.path.join(pred_dir, image_name)\n",
    "        if os.path.exists(image_path):\n",
    "            image_path_list.append(image_path)\n",
    "            label_path_list.append(label_path)\n",
    "        else:\n",
    "            basename = os.path.splitext(image_name)[0]\n",
    "            image_path = os.path.join(pred_dir, basename+'.jpg')\n",
    "#             print(image_path)\n",
    "            if os.path.exists(image_path):\n",
    "                image_path_list.append(image_path)\n",
    "                label_path_list.append(label_path)  \n",
    "    print(len(image_path_list))\n",
    "    if len(image_path_list) == 0:\n",
    "        print(pred_dir)            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for dataset_name,gt_dataset_dir in zip(dataset_names,gt_datasets):\n",
    "    for model_name,pred_model_dir in zip(model_names,pred_model_dirs):\n",
    "        pred_dataset_dir = os.path.join(pred_model_dir ,dataset_name)\n",
    "        check_dir_exists(gt_dataset_dir, pred_dataset_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_mae_and_max_f_and_s(gt_dir,pred_dir):\n",
    "    # get max-s\n",
    "    loader = EvalDataset(pred_dir, gt_dir)\n",
    "    thread = Eval_thread(loader, True)\n",
    "    s_dict = thread.run()\n",
    "    print(s_dict)\n",
    "    \n",
    "    image_path_list = list()\n",
    "    label_path_list = list()\n",
    "    \n",
    "    for image_name in os.listdir(gt_dir):\n",
    "        label_path = os.path.join(gt_dir, image_name)\n",
    "        image_path = os.path.join(pred_dir, image_name)\n",
    "        if os.path.exists(image_path):\n",
    "            image_path_list.append(image_path)\n",
    "            label_path_list.append(label_path)\n",
    "        else:\n",
    "            basename = os.path.splitext(image_name)[0]\n",
    "            image_path = os.path.join(pred_dir, basename+'.jpg')\n",
    "#             print(image_path)\n",
    "            if os.path.exists(image_path):\n",
    "                image_path_list.append(image_path)\n",
    "                label_path_list.append(label_path)  \n",
    "                \n",
    "    with open(lst_file_path,'w',encoding=\"utf-8\") as fp:\n",
    "        for image_path,label_path in zip(image_path_list,label_path_list):\n",
    "            img_obj = imread(image_path)\n",
    "            gt_obj = imread(label_path)\n",
    "            if img_obj is not None and gt_obj is not None and \\\n",
    "            gt_obj.shape == img_obj.shape:\n",
    "                fp.write(image_path + ' ' + label_path +'\\n')\n",
    "    cmd_str = salmetric_cmd_path + ' ' + lst_file_path + ' 10'\n",
    "    rd_lines = popen(cmd_str).readlines()\n",
    "    rd_lines = rd_lines[-4:]\n",
    "    eval_result_dict = dict()\n",
    "    eval_result_dict['Max F-measre'] = 0.0\n",
    "    eval_result_dict['MAE'] = 0.0\n",
    "    eval_result_dict.update(s_dict)\n",
    "    if len(rd_lines) == 4:\n",
    "        rd_lines = rd_lines[0:len(rd_lines):len(rd_lines)-1]\n",
    "        if len(rd_lines) == 2:\n",
    "            for line in rd_lines:\n",
    "                w_data = line.strip()\n",
    "                w_data = w_data.split(':')\n",
    "                if len(w_data) == 2:\n",
    "                     eval_result_dict[w_data[0].strip()] = float(w_data[1].strip())\n",
    "    return eval_result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for dataset_name,gt_dataset_dir in zip(dataset_names,gt_datasets):\n",
    "    for model_name,pred_model_dir in zip(model_names,pred_model_dirs):\n",
    "        print(model_name,dataset_name,)\n",
    "        pred_dataset_dir = os.path.join(pred_model_dir ,dataset_name)\n",
    "        ret = get_mae_and_max_f_and_s(gt_dataset_dir, pred_dataset_dir)\n",
    "        print(ret)\n",
    "        try:\n",
    "            res_dict[model_name][dataset_name] = ret\n",
    "        except:\n",
    "            res_dict[model_name] = dict()\n",
    "            res_dict[model_name][dataset_name] = ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('spl2.pkl','wb') as f:\n",
    "    pickle.dump(res_dict,f)\n",
    "# import pickle\n",
    "# with open('spl2.pkl','rb') as f:\n",
    "#     res_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# id_list = ['PoolNet','BASNet','BRN','MLMSNet','PAGE-Net','PAGRN']\n",
    "id_list = model_names\n",
    "dataset_list = ['ECSSD','PASCAL-S','DUT-OMRON','HKU-IS','SOD','DUTS-TE']\n",
    "# dataset_list = ['DUTS-7-8','ECSSD','PASCAL-S','DUT-OMRON','HKU-IS','SOD','DUTS-TE']\n",
    "# dataset_list = ['DUTS-TE',]\n",
    "# dataset_list = dataset_names\n",
    "metric_list = ['Max F-measre','MAE','max-S']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = dict()\n",
    "dataframe['ID'] = id_list\n",
    "for dataset in dataset_list:\n",
    "    for metric in metric_list:\n",
    "        dataframe[dataset+' '+metric] = [res_dict[id_name][dataset][metric] for id_name in id_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "new_dataframe = pd.DataFrame(dataframe)\n",
    "print(new_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataframe.to_excel('./1000-2000.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 删除临时文件\n",
    "os.unlink(lst_file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
